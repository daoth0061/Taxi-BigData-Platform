#Minio
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=admin
spark.hadoop.fs.s3a.secret.key=12345678
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem

# # Iceberg (so Spark Thrift/Superset can query lakehouse.* tables)
# spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0
# spark.sql.catalog.lakehouse=org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.lakehouse.type=hadoop
# spark.sql.catalog.lakehouse.warehouse=s3a://lakehouse/warehouse

# # Delta Lake
# spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,io.delta.sql.DeltaSparkSessionExtension
# spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog 

# Extra JARs for S3A filesystem and Iceberg
spark.driver.extraClassPath=/opt/spark/extra-jars/*
spark.executor.extraClassPath=/opt/spark/extra-jars/*
spark.jars=/opt/spark/extra-jars/hadoop-aws-3.3.4.jar,/opt/spark/extra-jars/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/extra-jars/iceberg-spark-runtime-3.5_2.12-1.6.0.jar

# Iceberg catalog config (for Spark jobs reading Iceberg tables)
# Note: Removed spark.sql.extensions to avoid ViewUtil$ error with Thrift Server
# spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.0
# spark.sql.catalog.lakehouse=org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.lakehouse.type=hive
# spark.sql.catalog.lakehouse.uri=thrift://hive-metastore:9083
# spark.sql.catalog.lakehouse.warehouse=s3a://lakehouse/warehouse
# spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions 